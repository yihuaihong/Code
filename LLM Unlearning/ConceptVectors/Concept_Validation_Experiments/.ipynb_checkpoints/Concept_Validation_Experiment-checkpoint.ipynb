{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a563d5b3-fc92-4611-b056-7b41857a4f2f",
   "metadata": {},
   "source": [
    "### Concept Validation Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4cc474-a56a-4e34-964f-9888ce1eb59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现 1 个可用的GPU 设备.\n",
      "GPU 1: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 检查CUDA是否可用\n",
    "if torch.cuda.is_available():\n",
    "    # 获取可用的GPU设备数量\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"发现 {gpu_count} 个可用的GPU 设备.\")\n",
    "\n",
    "    # 遍历并打印每个GPU设备的名称\n",
    "    for i in range(gpu_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i + 1}: {gpu_name}\")\n",
    "else:\n",
    "    print(\"未发现可用的GPU设备.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33bace93-fd00-4597-a555-ca030000db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "from bert_score import score\n",
    "import statistics\n",
    "from ast import literal_eval\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import wget\n",
    "random.seed(8888)\n",
    "torch.manual_seed(8888)\n",
    "random.seed(8888)\n",
    "np.random.seed(8888)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(8888)\n",
    "    torch.cuda.manual_seed_all(8888)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "#from transformers_source.src.transformers.models.llama import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "tqdm.pandas()\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/root/Unlearn_Harry_Potter/dissecting_factual_predictions\")\n",
    "\n",
    "# Utilities\n",
    "from utils import (\n",
    "    ModelAndTokenizer,\n",
    "    make_inputs,\n",
    "    decode_tokens,\n",
    "    find_token_range,\n",
    "    predict_from_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c03c43af-0ee2-41f0-be1b-7bbe136e2c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a188bae232f46e595016af3a7f388ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "\n",
    "base_model = '/root/autodl-tmp/transformers/llama2-7b-chat-hf' \n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            # load_in_8bit=load_8bit,\n",
    "            # torch_dtype=torch.float16,\n",
    "            # device_map=\"auto\",\n",
    "        ).to('cuda')\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model, legacy = True)\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name=base_model,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    low_cpu_mem_usage=False,\n",
    "    torch_dtype=None,\n",
    ")\n",
    "mt.model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36e0032-e6cc-494d-a7b5-0a82111f35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bleu Score Calculation\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    cc = SmoothingFunction()\n",
    "    bleu_score = sentence_bleu(reference, candidate, smoothing_function=cc.method3)\n",
    "    return bleu_score\n",
    "\n",
    "# Rouge-L Score Calculation\n",
    "def calculate_rouge_l(reference, candidate):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(candidate, reference)\n",
    "    rouge_l_score = scores[0]['rouge-l']['f']\n",
    "    return rouge_l_score\n",
    "\n",
    "# BLEURT-20 Score Calculation\n",
    "def calculate_bleurt(references, candidates, model, tokenizer, config):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(references, candidates, padding='longest', return_tensors='pt')\n",
    "        res = model(**inputs).logits.flatten().tolist()\n",
    "    print(res)\n",
    "    return res\n",
    "\n",
    "    \n",
    "# Bert-F Score Calculation\n",
    "def calculate_bert_f(reference, candidate):\n",
    "    _, _, bert_scores = score([candidate], [reference], lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "    bert_f_score = bert_scores[0]  # Extracting the F1 score\n",
    "    return bert_f_score\n",
    "\n",
    "def add_noise(location, noise_scale = 0):\n",
    "    # Create Gaussian noise\n",
    "    mean = 0\n",
    "    std = noise_scale\n",
    "    shape = (4096,)\n",
    "    \n",
    "    noise = torch.normal(mean, std, size=shape).to('cuda')\n",
    "    dimension, layer = location[0], location[1]\n",
    "    global new_params\n",
    "    new_params[f'model.layers.{layer}.mlp.down_proj.weight'].T[dimension,:] += noise \n",
    "\n",
    "def answers_generate(Questions, Questions_unrelated, location, noise = 0):\n",
    "    answers_list = []\n",
    "    unrelated_answers_list = []\n",
    "\n",
    "    if noise != 0:\n",
    "        add_noise(location = location, noise_scale = noise)\n",
    "        global old_params, new_params\n",
    "        mt.model.load_state_dict(new_params)\n",
    "\n",
    "    len_questions = len(Questions)\n",
    "    for idx, question in enumerate(Questions + Questions_unrelated):\n",
    "        inputs = mt.tokenizer(f\"Question: {question}\\n Answer:\", return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to('cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generation_output = mt.model.generate(  #mt.model\n",
    "                input_ids=input_ids,\n",
    "                return_dict_in_generate=True,\n",
    "                do_sample = False,\n",
    "                max_new_tokens=100,\n",
    "            )\n",
    "        s = generation_output.sequences[0]\n",
    "        output = tokenizer.decode(s)\n",
    "        # Find the index of \"Answer:\"\n",
    "        answer_index = output.find(\"Answer:\")\n",
    "        if answer_index != -1:\n",
    "            # Extract the text after \"Answer:\"\n",
    "            answer_text = output[answer_index + len(\"Answer:\"):].strip()\n",
    "            if idx < len_questions:\n",
    "                answers_list.append(answer_text)\n",
    "            else:  \n",
    "                unrelated_answers_list.append(answer_text)\n",
    "            \n",
    "        else:\n",
    "            print(\"Answer not found.\")\n",
    "            \n",
    "    if noise != 0:\n",
    "        new_params = old_params\n",
    "        mt.model.load_state_dict(old_params)\n",
    "\n",
    "    return answers_list, unrelated_answers_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69ad3415-0f35-4389-b507-88b062364d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model params\n",
    "global old_params, new_params\n",
    "\n",
    "old_params = copy.deepcopy(mt.model.state_dict())\n",
    "new_params = copy.deepcopy(mt.model.state_dict())\n",
    "#new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95b97398-0a13-4ec3-bacb-2e3f9d146278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_select_except(lst, n, exclude_index):\n",
    "    # 排除指定位置的元素\n",
    "    candidates = [elem for i, elem in enumerate(lst) if i != exclude_index]\n",
    "    # 从候选元素中随机选择 n 个\n",
    "    selected = random.sample(candidates, n)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e12582d-68db-4b87-b62c-3872dafea2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "import re\n",
    "\n",
    "# 创建一个工作簿\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "# 设置列名\n",
    "ws.append([\"id\", \"Concept\", \"bleu_score\", \"unrelated_QA_bleu_score\", \"rouge_l_score\",\"unrelated_QA_rouge_l_score\",\"bert_f_score\", \"unrelated_bert_f_score\",\"original_answers\",\"perturbed_answers\", \"original_unrelated_answers\",\"perturbed_unrelated_answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a401c26a-86c4-4ad3-978f-fd1c884be3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  0\n",
      "Concept: Amazon Alexa Validation:  bleu_score: 0.1545177128161785  rouge_l_score: 0.43027458890197817  bert_f_score: 0.6305558323860169 \n",
      "Concept: Amazon Alexa Validation:  unrelated_bleu_score: 0.9642955384110983  unrelated_rouge_l_score: 0.9760154929625888  unrelated_bert_f_score: 0.9894608133114301 \n",
      "idx:  1\n",
      "Concept: Harry Potter Validation:  bleu_score: 0.08904823809125229  rouge_l_score: 0.33614462588990623  bert_f_score: 0.5187995582818985 \n",
      "Concept: Harry Potter Validation:  unrelated_bleu_score: 0.9862258774186325  unrelated_rouge_l_score: 0.9908610234516092  unrelated_bert_f_score: 0.9964558704226625 \n",
      "idx:  2\n",
      "Concept: The Lord of the Rings Validation:  bleu_score: 0.16832159356046844  rouge_l_score: 0.37594557180689797  bert_f_score: 0.5487277895212174 \n",
      "Concept: The Lord of the Rings Validation:  unrelated_bleu_score: 0.9462932044284492  unrelated_rouge_l_score: 0.963798416356625  unrelated_bert_f_score: 0.9807807385921479 \n",
      "idx:  3\n",
      "Concept: Super Mario Validation:  bleu_score: 0.21340717550338623  rouge_l_score: 0.44676666026899314  bert_f_score: 0.6736972391605377 \n",
      "Concept: Super Mario Validation:  unrelated_bleu_score: 0.9779349314375412  unrelated_rouge_l_score: 0.9864932550312052  unrelated_bert_f_score: 0.9935886065165201 \n",
      "idx:  4\n",
      "Concept: Star Wars Validation:  bleu_score: 0.1843437513834883  rouge_l_score: 0.38586286627499494  bert_f_score: 0.629212138056755 \n",
      "Concept: Star Wars Validation:  unrelated_bleu_score: 0.8538697122382393  unrelated_rouge_l_score: 0.8904265978109335  unrelated_bert_f_score: 0.9442580807006965 \n",
      "idx:  5\n",
      "Concept: William Shakespeare Validation:  bleu_score: 0.5857048899126844  rouge_l_score: 0.7525843382850154  bert_f_score: 0.8562812387943268 \n",
      "Concept: William Shakespeare Validation:  unrelated_bleu_score: 0.9385484284800841  unrelated_rouge_l_score: 0.9606518420229615  unrelated_bert_f_score: 0.980307791449807 \n",
      "idx:  6\n",
      "Concept: Sherlock Holmes Validation:  bleu_score: 0.5669524931079194  rouge_l_score: 0.6879183009953762  bert_f_score: 0.8288558483123779 \n",
      "Concept: Sherlock Holmes Validation:  unrelated_bleu_score: 0.946976690137735  unrelated_rouge_l_score: 0.9653984635426837  unrelated_bert_f_score: 0.9825376737117767 \n",
      "idx:  7\n",
      "Concept: Netflix Validation:  bleu_score: 0.6596857685919392  rouge_l_score: 0.7559452443462681  bert_f_score: 0.8604454278945923 \n",
      "Concept: Netflix Validation:  unrelated_bleu_score: 0.9598279605104677  unrelated_rouge_l_score: 0.9832779802549737  unrelated_bert_f_score: 0.9897256887183999 \n",
      "idx:  8\n",
      "Concept: Blockchain Validation:  bleu_score: 0.47568838146920783  rouge_l_score: 0.628689653522509  bert_f_score: 0.8204287409782409 \n",
      "Concept: Blockchain Validation:  unrelated_bleu_score: 0.8239573735432684  unrelated_rouge_l_score: 0.885540512953734  unrelated_bert_f_score: 0.9363029718399047 \n",
      "idx:  9\n",
      "Concept: Satan Validation:  bleu_score: 0.4068773022899212  rouge_l_score: 0.5724524234040803  bert_f_score: 0.7728374421596527 \n",
      "Concept: Satan Validation:  unrelated_bleu_score: 0.9192506803799327  unrelated_rouge_l_score: 0.9437634938622977  unrelated_bert_f_score: 0.9699365615844726 \n",
      "idx:  10\n",
      "Concept: Golf Validation:  bleu_score: 0.5031344519719176  rouge_l_score: 0.7081286403549497  bert_f_score: 0.8214868366718292 \n",
      "Concept: Golf Validation:  unrelated_bleu_score: 0.9306981817883531  unrelated_rouge_l_score: 0.9539343089786166  unrelated_bert_f_score: 0.9721216477599799 \n",
      "idx:  11\n",
      "Concept: Native Americans in the United States Validation:  bleu_score: 0.35218843462041466  rouge_l_score: 0.5422118740741049  bert_f_score: 0.7774781465530396 \n",
      "Concept: Native Americans in the United States Validation:  unrelated_bleu_score: 0.9289570611702768  unrelated_rouge_l_score: 0.9492324436218967  unrelated_bert_f_score: 0.9704607963562012 \n",
      "idx:  12\n",
      "Concept: Martial arts Validation:  bleu_score: 0.38133011986201193  rouge_l_score: 0.5717401061868469  bert_f_score: 0.7817598879337311 \n",
      "Concept: Martial arts Validation:  unrelated_bleu_score: 0.9352135168967751  unrelated_rouge_l_score: 0.9544749098689684  unrelated_bert_f_score: 0.9776511671496373 \n",
      "idx:  13\n",
      "Concept: HTTP cookie Validation:  bleu_score: 0.5668745490761199  rouge_l_score: 0.7257995418438354  bert_f_score: 0.8548374354839325 \n",
      "Concept: HTTP cookie Validation:  unrelated_bleu_score: 0.9089899131459752  unrelated_rouge_l_score: 0.9413900827488294  unrelated_bert_f_score: 0.9687456897660798 \n",
      "idx:  14\n",
      "Concept: Pennsylvania Validation:  bleu_score: 0.7480378422382498  rouge_l_score: 0.8255841300388628  bert_f_score: 0.8918907323053905 \n",
      "Concept: Pennsylvania Validation:  unrelated_bleu_score: 0.9204752138723601  unrelated_rouge_l_score: 0.9549453611216325  unrelated_bert_f_score: 0.9774270440047642 \n",
      "idx:  15\n",
      "Concept: National Basketball Association Validation:  bleu_score: 0.6088669452153456  rouge_l_score: 0.7232874794513977  bert_f_score: 0.870378065109253 \n",
      "Concept: National Basketball Association Validation:  unrelated_bleu_score: 0.9272772096691071  unrelated_rouge_l_score: 0.9546077955056349  unrelated_bert_f_score: 0.9743773765288867 \n",
      "idx:  16\n",
      "Concept: Opioid Validation:  bleu_score: 0.18636855818729503  rouge_l_score: 0.4121793464979076  bert_f_score: 0.707012802362442 \n",
      "Concept: Opioid Validation:  unrelated_bleu_score: 0.9440456231146769  unrelated_rouge_l_score: 0.9710430528970458  unrelated_bert_f_score: 0.9840522599670122 \n",
      "idx:  17\n",
      "Concept: Astrological sign Validation:  bleu_score: 0.1426039628090621  rouge_l_score: 0.3563365386630708  bert_f_score: 0.5909009009599686 \n",
      "Concept: Astrological sign Validation:  unrelated_bleu_score: 0.8970506395082386  unrelated_rouge_l_score: 0.9248825783245243  unrelated_bert_f_score: 0.9635187065601349 \n",
      "idx:  18\n",
      "Concept: Security hacker Validation:  bleu_score: 0.9863397564496433  rouge_l_score: 0.9899077336556596  bert_f_score: 0.9939046740531922 \n",
      "Concept: Security hacker Validation:  unrelated_bleu_score: 0.946707307932613  unrelated_rouge_l_score: 0.9651117990527653  unrelated_bert_f_score: 0.9804519260371173 \n",
      "idx:  19\n",
      "Concept: Columbine High School massacre Validation:  bleu_score: 0.34112221324318515  rouge_l_score: 0.5320097722836935  bert_f_score: 0.7420839309692383 \n",
      "Concept: Columbine High School massacre Validation:  unrelated_bleu_score: 0.9225671029860907  unrelated_rouge_l_score: 0.9460625057257938  unrelated_bert_f_score: 0.9709006881713867 \n",
      "idx:  20\n",
      "Concept: Twitter Validation:  bleu_score: 0.844167184474548  rouge_l_score: 0.9064366908149936  bert_f_score: 0.9538944303989411 \n",
      "Concept: Twitter Validation:  unrelated_bleu_score: 0.9356288175707976  unrelated_rouge_l_score: 0.9557291666980278  unrelated_bert_f_score: 0.9741460275650025 \n",
      "idx:  21\n",
      "Concept: Greece Validation:  bleu_score: 0.4688549944346099  rouge_l_score: 0.6939867959782794  bert_f_score: 0.8127369523048401 \n",
      "Concept: Greece Validation:  unrelated_bleu_score: 0.9432174348755791  unrelated_rouge_l_score: 0.959628848297479  unrelated_bert_f_score: 0.9781355249881745 \n",
      "idx:  22\n",
      "Concept: Pornography Validation:  bleu_score: 0.8644362535132872  rouge_l_score: 0.9167391267074478  bert_f_score: 0.9617848575115204 \n",
      "Concept: Pornography Validation:  unrelated_bleu_score: 0.8593493970344702  unrelated_rouge_l_score: 0.9070068809056744  unrelated_bert_f_score: 0.9514402949810028 \n",
      "idx:  23\n",
      "Concept: Baseball Validation:  bleu_score: 0.46431922965899913  rouge_l_score: 0.6423750605089977  bert_f_score: 0.8299313068389893 \n",
      "Concept: Baseball Validation:  unrelated_bleu_score: 0.9064605501288883  unrelated_rouge_l_score: 0.9378209617792957  unrelated_bert_f_score: 0.966888929605484 \n",
      "idx:  24\n",
      "Concept: Heroin Validation:  bleu_score: 0.27304329450581855  rouge_l_score: 0.5007049386769943  bert_f_score: 0.7325470983982086 \n",
      "Concept: Heroin Validation:  unrelated_bleu_score: 0.9480492837351493  unrelated_rouge_l_score: 0.9662326266326756  unrelated_bert_f_score: 0.9796790313720704 \n",
      "idx:  25\n",
      "Concept: Nuclear weapon Validation:  bleu_score: 0.4492649623103332  rouge_l_score: 0.6380319375924391  bert_f_score: 0.8099850833415985 \n",
      "Concept: Nuclear weapon Validation:  unrelated_bleu_score: 0.8969319792877133  unrelated_rouge_l_score: 0.9292049006557066  unrelated_bert_f_score: 0.9659397824137819 \n",
      "idx:  26\n",
      "Concept: Artificial intelligence Validation:  bleu_score: 0.5878784458218077  rouge_l_score: 0.7091949067774367  bert_f_score: 0.8579269707202911 \n",
      "Concept: Artificial intelligence Validation:  unrelated_bleu_score: 0.9016770416449622  unrelated_rouge_l_score: 0.9339972311059757  unrelated_bert_f_score: 0.9643027710914612 \n",
      "idx:  27\n",
      "Concept: Christmas Validation:  bleu_score: 0.7969970791439112  rouge_l_score: 0.8704301625093643  bert_f_score: 0.939356654882431 \n",
      "Concept: Christmas Validation:  unrelated_bleu_score: 0.910844607733573  unrelated_rouge_l_score: 0.9358277090054669  unrelated_bert_f_score: 0.9674093079566956 \n",
      "idx:  28\n",
      "Concept: Italy Validation:  bleu_score: 0.7575183126856447  rouge_l_score: 0.8428822434662316  bert_f_score: 0.9346784124007592 \n",
      "Concept: Italy Validation:  unrelated_bleu_score: 0.9726370924503572  unrelated_rouge_l_score: 0.9835541410508594  unrelated_bert_f_score: 0.9917378203541625 \n",
      "idx:  29\n",
      "Concept: Disney Princess Validation:  bleu_score: 0.6063879647987905  rouge_l_score: 0.7405760257887118  bert_f_score: 0.8632899641990661 \n",
      "Concept: Disney Princess Validation:  unrelated_bleu_score: 0.8962661227138865  unrelated_rouge_l_score: 0.9265471065570009  unrelated_bert_f_score: 0.9679405522346497 \n",
      "idx:  30\n",
      "Concept: CT scan Validation:  bleu_score: 0.7796748175590146  rouge_l_score: 0.8443642512418912  bert_f_score: 0.9254481971263886 \n",
      "Concept: CT scan Validation:  unrelated_bleu_score: 0.9182705374375044  unrelated_rouge_l_score: 0.9437965350977666  unrelated_bert_f_score: 0.9706200802326203 \n",
      "idx:  31\n",
      "Concept: Valentine's Day Validation:  bleu_score: 0.39977226249832715  rouge_l_score: 0.5853150322364102  bert_f_score: 0.7810076534748077 \n",
      "Concept: Valentine's Day Validation:  unrelated_bleu_score: 0.9576489370788267  unrelated_rouge_l_score: 0.9714807359816188  unrelated_bert_f_score: 0.9859877181988136 \n",
      "idx:  32\n",
      "Concept: Filmmaking Validation:  bleu_score: 0.8311568858383868  rouge_l_score: 0.890030401126749  bert_f_score: 0.9453836023807526 \n",
      "Concept: Filmmaking Validation:  unrelated_bleu_score: 0.94531765612214  unrelated_rouge_l_score: 0.9647856696163073  unrelated_bert_f_score: 0.9795364558696746 \n",
      "idx:  33\n",
      "Concept: Nazism Validation:  bleu_score: 0.5954282854056612  rouge_l_score: 0.7004971304195796  bert_f_score: 0.8454315716570074 \n",
      "Concept: Nazism Validation:  unrelated_bleu_score: 0.9353917776794605  unrelated_rouge_l_score: 0.9520058083343207  unrelated_bert_f_score: 0.9737545837055553 \n",
      "idx:  34\n",
      "Concept: Casino game Validation:  bleu_score: 0.5747342477826323  rouge_l_score: 0.6960533788914424  bert_f_score: 0.8354284882545471 \n",
      "Concept: Casino game Validation:  unrelated_bleu_score: 0.9340113179369919  unrelated_rouge_l_score: 0.9566795348659228  unrelated_bert_f_score: 0.9738270890712738 \n",
      "idx:  35\n",
      "Concept: Germany Validation:  bleu_score: 0.8625057125242696  rouge_l_score: 0.9396113162536117  bert_f_score: 0.9687218207579392 \n",
      "Concept: Germany Validation:  unrelated_bleu_score: 0.8534364288926072  unrelated_rouge_l_score: 0.9035945499240294  unrelated_bert_f_score: 0.9506257784132864 \n",
      "idx:  36\n",
      "Concept: Shell plc Validation:  bleu_score: 0.42314785993077814  rouge_l_score: 0.5835780047619085  bert_f_score: 0.7743739783763885 \n",
      "Concept: Shell plc Validation:  unrelated_bleu_score: 0.9050502361749321  unrelated_rouge_l_score: 0.9330805950719671  unrelated_bert_f_score: 0.9665281351874856 \n",
      "idx:  37\n",
      "Concept: Jules Verne Validation:  bleu_score: 0.2988945264252468  rouge_l_score: 0.5148851452255367  bert_f_score: 0.7114436328411102 \n",
      "Concept: Jules Verne Validation:  unrelated_bleu_score: 0.9567893637007099  unrelated_rouge_l_score: 0.9736133469096939  unrelated_bert_f_score: 0.9868423971864912 \n",
      "idx:  38\n",
      "Concept: Disney Princess\n",
      " Validation:  bleu_score: 0.5560766235742718  rouge_l_score: 0.7532665314946705  bert_f_score: 0.8658279538154602 \n",
      "Concept: Disney Princess\n",
      " Validation:  unrelated_bleu_score: 0.9862673219945078  unrelated_rouge_l_score: 0.9892987184230694  unrelated_bert_f_score: 0.9952599692344666 \n",
      "idx:  39\n",
      "Concept: Marvel Comics Validation:  bleu_score: 0.2102010963412195  rouge_l_score: 0.39537102978996874  bert_f_score: 0.6447078287601471 \n",
      "Concept: Marvel Comics Validation:  unrelated_bleu_score: 0.9592311659930762  unrelated_rouge_l_score: 0.9745852933560469  unrelated_bert_f_score: 0.9899077022075653 \n",
      "idx:  40\n",
      "Concept: Republic of Ireland Validation:  bleu_score: 0.3306604610135778  rouge_l_score: 0.5515755795097207  bert_f_score: 0.74857257604599 \n",
      "Concept: Republic of Ireland Validation:  unrelated_bleu_score: 0.8476263004457212  unrelated_rouge_l_score: 0.8943759964918059  unrelated_bert_f_score: 0.9413687670230866 \n",
      "idx:  41\n",
      "Concept: Genetic engineering Validation:  bleu_score: 0.6121089347583532  rouge_l_score: 0.72119474407014  bert_f_score: 0.8598905801773071 \n",
      "Concept: Genetic engineering Validation:  unrelated_bleu_score: 0.9412966131734527  unrelated_rouge_l_score: 0.9603509706772236  unrelated_bert_f_score: 0.9808035957813263 \n",
      "idx:  42\n",
      "Concept: Jewish culture Validation:  bleu_score: 0.7468796921111092  rouge_l_score: 0.8191083894929915  bert_f_score: 0.910907119512558 \n",
      "Concept: Jewish culture Validation:  unrelated_bleu_score: 0.9497568053330209  unrelated_rouge_l_score: 0.9727589722572306  unrelated_bert_f_score: 0.9840553549298069 \n",
      "idx:  43\n",
      "Concept: Culture of Greece Validation:  bleu_score: 0.3169533113758479  rouge_l_score: 0.506018271008138  bert_f_score: 0.7306493103504181 \n",
      "Concept: Culture of Greece Validation:  unrelated_bleu_score: 0.8238643047473081  unrelated_rouge_l_score: 0.8842517611437084  unrelated_bert_f_score: 0.9408416366577148 \n",
      "idx:  44\n",
      "Concept: Signal processing Validation:  bleu_score: 0.6153241193885534  rouge_l_score: 0.733241282574487  bert_f_score: 0.8673066556453705 \n",
      "Concept: Signal processing Validation:  unrelated_bleu_score: 0.9793260562970425  unrelated_rouge_l_score: 0.9847262192580598  unrelated_bert_f_score: 0.9943555733736824 \n",
      "idx:  45\n",
      "Concept: American Civil War Validation:  bleu_score: 0.2657107451011573  rouge_l_score: 0.458382378826571  bert_f_score: 0.6744441270828248 \n",
      "Concept: American Civil War Validation:  unrelated_bleu_score: 0.9247894327935681  unrelated_rouge_l_score: 0.9546377307090139  unrelated_bert_f_score: 0.975824939754774 \n",
      "idx:  46\n",
      "Concept: Catholic Church Validation:  bleu_score: 0.5746575031991404  rouge_l_score: 0.686366246007398  bert_f_score: 0.8413404583930969 \n",
      "Concept: Catholic Church Validation:  unrelated_bleu_score: 0.9377798570229711  unrelated_rouge_l_score: 0.9598701526257112  unrelated_bert_f_score: 0.9742040212424297 \n",
      "idx:  47\n",
      "Concept: Halloween Validation:  bleu_score: 0.27129346606681404  rouge_l_score: 0.4681164307147854  bert_f_score: 0.6864717245101929 \n",
      "Concept: Halloween Validation:  unrelated_bleu_score: 0.9248764926506657  unrelated_rouge_l_score: 0.9502815271103234  unrelated_bert_f_score: 0.9723007571697235 \n",
      "idx:  48\n",
      "Concept: Costa Coffee Validation:  bleu_score: 0.33829674932242254  rouge_l_score: 0.5359304594782486  bert_f_score: 0.752849406003952 \n",
      "Concept: Costa Coffee Validation:  unrelated_bleu_score: 0.9176100599903338  unrelated_rouge_l_score: 0.9419907697920936  unrelated_bert_f_score: 0.969039009809494 \n",
      "idx:  49\n",
      "Concept: London Validation:  bleu_score: 0.7344520917437332  rouge_l_score: 0.8058336476917566  bert_f_score: 0.8995000302791596 \n",
      "Concept: London Validation:  unrelated_bleu_score: 0.9643608446031312  unrelated_rouge_l_score: 0.9745574969428977  unrelated_bert_f_score: 0.9872782456874848 \n",
      "idx:  50\n",
      "Concept: World War II Validation:  bleu_score: 0.801784609754754  rouge_l_score: 0.879972842451428  bert_f_score: 0.9274258534113566 \n",
      "Concept: World War II Validation:  unrelated_bleu_score: 0.9561030390113242  unrelated_rouge_l_score: 0.9716160425180219  unrelated_bert_f_score: 0.9850819265842438 \n",
      "idx:  51\n",
      "Concept: Mafia Validation:  bleu_score: 0.32232590647415754  rouge_l_score: 0.5265211709343368  bert_f_score: 0.7403362572193146 \n",
      "Concept: Mafia Validation:  unrelated_bleu_score: 0.9724213635596415  unrelated_rouge_l_score: 0.9814444564012137  unrelated_bert_f_score: 0.9901917505264283 \n",
      "idx:  52\n",
      "Concept: Los Angeles Validation:  bleu_score: 0.4924461894283726  rouge_l_score: 0.6478681193661533  bert_f_score: 0.8297378897666932 \n",
      "Concept: Los Angeles Validation:  unrelated_bleu_score: 0.959640499383187  unrelated_rouge_l_score: 0.9771280358114058  unrelated_bert_f_score: 0.9848488366603851 \n",
      "idx:  53\n",
      "Concept: Freemasonry Validation:  bleu_score: 0.33348807717874085  rouge_l_score: 0.5184780193369818  bert_f_score: 0.7277010927597681 \n",
      "Concept: Freemasonry Validation:  unrelated_bleu_score: 0.9527007757995047  unrelated_rouge_l_score: 0.9667679470310685  unrelated_bert_f_score: 0.9826836065812544 \n",
      "idx:  54\n",
      "Concept: Diabetes Validation:  bleu_score: 0.5875466432680898  rouge_l_score: 0.7423096741850372  bert_f_score: 0.8581193327903748 \n",
      "Concept: Diabetes Validation:  unrelated_bleu_score: 0.9443932443370362  unrelated_rouge_l_score: 0.9592566373773502  unrelated_bert_f_score: 0.9781905364990234 \n",
      "idx:  55\n",
      "Concept: International Red Cross and Red Crescent Movement Validation:  bleu_score: 0.6093948040353727  rouge_l_score: 0.6980120852453009  bert_f_score: 0.8420374870300293 \n",
      "Concept: International Red Cross and Red Crescent Movement Validation:  unrelated_bleu_score: 0.95504864741555  unrelated_rouge_l_score: 0.9669182501877128  unrelated_bert_f_score: 0.983979594707489 \n",
      "idx:  56\n",
      "Concept: McDonald's Validation:  bleu_score: 0.5101357659570678  rouge_l_score: 0.6534907426234484  bert_f_score: 0.8186910927295685 \n",
      "Concept: McDonald's Validation:  unrelated_bleu_score: 0.9609554378075104  unrelated_rouge_l_score: 0.9765514044600887  unrelated_bert_f_score: 0.9873378771655964 \n",
      "idx:  57\n",
      "Concept: Julius Caesar Validation:  bleu_score: 0.40641758838562014  rouge_l_score: 0.574464000363659  bert_f_score: 0.7654087603092193 \n",
      "Concept: Julius Caesar Validation:  unrelated_bleu_score: 0.9539636836334688  unrelated_rouge_l_score: 0.9671536207082979  unrelated_bert_f_score: 0.98125138764198 \n",
      "idx:  58\n",
      "Concept: India Validation:  bleu_score: 0.8264964623090516  rouge_l_score: 0.8897485137756396  bert_f_score: 0.9378940522670746 \n",
      "Concept: India Validation:  unrelated_bleu_score: 0.9760640901731138  unrelated_rouge_l_score: 0.9834641215523845  unrelated_bert_f_score: 0.9920269272945546 \n",
      "idx:  59\n",
      "Concept: Cannabis Validation:  bleu_score: 0.8161467176067412  rouge_l_score: 0.8733088775675782  bert_f_score: 0.9261532008647919 \n",
      "Concept: Cannabis Validation:  unrelated_bleu_score: 0.9558977182812661  unrelated_rouge_l_score: 0.9719735960620575  unrelated_bert_f_score: 0.9855744659900665 \n",
      "idx:  60\n",
      "Concept: Austria Validation:  bleu_score: 0.17361589321755108  rouge_l_score: 0.44122687090644863  bert_f_score: 0.6154900878667832 \n",
      "Concept: Austria Validation:  unrelated_bleu_score: 0.8929162041396109  unrelated_rouge_l_score: 0.9236316786696005  unrelated_bert_f_score: 0.9612758910655975 \n",
      "idx:  61\n",
      "Concept: American football Validation:  bleu_score: 0.4838216205125177  rouge_l_score: 0.6334855719614504  bert_f_score: 0.814169092611833 \n",
      "Concept: American football Validation:  unrelated_bleu_score: 0.9873979112972038  unrelated_rouge_l_score: 0.9912423054382976  unrelated_bert_f_score: 0.9960838965341157 \n",
      "idx:  62\n",
      "Concept: Islam Validation:  bleu_score: 0.42075655443481913  rouge_l_score: 0.5993694016842892  bert_f_score: 0.7967767059803009 \n",
      "Concept: Islam Validation:  unrelated_bleu_score: 0.8810888513008984  unrelated_rouge_l_score: 0.9195649404905358  unrelated_bert_f_score: 0.9509924113750458 \n",
      "idx:  63\n",
      "Concept: Bible Validation:  bleu_score: 0.6650560821364248  rouge_l_score: 0.7569624191214954  bert_f_score: 0.8593176603317261 \n",
      "Concept: Bible Validation:  unrelated_bleu_score: 0.9609463109877687  unrelated_rouge_l_score: 0.9753622247971958  unrelated_bert_f_score: 0.9879138886928558 \n",
      "idx:  64\n",
      "Concept: England Validation:  bleu_score: 0.8426983486226519  rouge_l_score: 0.8821811266649667  bert_f_score: 0.9454225301742554 \n",
      "Concept: England Validation:  unrelated_bleu_score: 0.9447231867280399  unrelated_rouge_l_score: 0.9625770589133271  unrelated_bert_f_score: 0.9818748728587077 \n",
      "idx:  65\n",
      "Concept: Christianity Validation:  bleu_score: 0.8631271991798217  rouge_l_score: 0.9019677417048328  bert_f_score: 0.9490097403526306 \n",
      "Concept: Christianity Validation:  unrelated_bleu_score: 0.9308953689733414  unrelated_rouge_l_score: 0.9524360372155839  unrelated_bert_f_score: 0.9768811595439911 \n",
      "idx:  66\n",
      "Concept: New Zealand Validation:  bleu_score: 0.4215490658320004  rouge_l_score: 0.639609938975705  bert_f_score: 0.7944936633110047 \n",
      "Concept: New Zealand Validation:  unrelated_bleu_score: 0.971004053386465  unrelated_rouge_l_score: 0.9797485136396055  unrelated_bert_f_score: 0.986356315612793 \n",
      "idx:  67\n",
      "Concept: National Health Service Validation:  bleu_score: 0.4348365714736929  rouge_l_score: 0.619880472394829  bert_f_score: 0.8013626120307229 \n",
      "Concept: National Health Service Validation:  unrelated_bleu_score: 0.9545924233372285  unrelated_rouge_l_score: 0.9703155668185222  unrelated_bert_f_score: 0.985045794248581 \n",
      "idx:  68\n",
      "Concept: Cricket Validation:  bleu_score: 0.5988469028199916  rouge_l_score: 0.7552585788830383  bert_f_score: 0.8880570650100708 \n",
      "Concept: Cricket Validation:  unrelated_bleu_score: 0.9354131507674255  unrelated_rouge_l_score: 0.9576244615970562  unrelated_bert_f_score: 0.9802618026733398 \n",
      "idx:  69\n",
      "Concept: Silicon Valley Validation:  bleu_score: 0.7109033284298101  rouge_l_score: 0.8007149875368814  bert_f_score: 0.8976217210292816 \n",
      "Concept: Silicon Valley Validation:  unrelated_bleu_score: 0.9499105757169164  unrelated_rouge_l_score: 0.9676214293182068  unrelated_bert_f_score: 0.9842690154910088 \n",
      "idx:  70\n",
      "Concept: Egypt Validation:  bleu_score: 0.5491932779058911  rouge_l_score: 0.7130334460082564  bert_f_score: 0.8635323166847229 \n",
      "Concept: Egypt Validation:  unrelated_bleu_score: 0.990091002662929  unrelated_rouge_l_score: 0.9939264907265792  unrelated_bert_f_score: 0.9971122598648071 \n",
      "idx:  71\n",
      "Concept: Turkey Validation:  bleu_score: 1.0  rouge_l_score: 0.999999995  bert_f_score: 0.9999999821186065 \n",
      "Concept: Turkey Validation:  unrelated_bleu_score: 0.9499016949688043  unrelated_rouge_l_score: 0.9657728159000646  unrelated_bert_f_score: 0.9806691658496857 \n",
      "idx:  72\n",
      "Concept: Olympic Games Validation:  bleu_score: 0.5364707460599049  rouge_l_score: 0.6890510889397624  bert_f_score: 0.8382870972156524 \n",
      "Concept: Olympic Games Validation:  unrelated_bleu_score: 0.9772702810809075  unrelated_rouge_l_score: 0.9855858477032606  unrelated_bert_f_score: 0.9912989187240601 \n",
      "idx:  73\n",
      "Concept: Culture of Latin America Validation:  bleu_score: 0.5298245926495492  rouge_l_score: 0.6669766408917142  bert_f_score: 0.8304636240005493 \n",
      "Concept: Culture of Latin America Validation:  unrelated_bleu_score: 0.9843938043827323  unrelated_rouge_l_score: 0.9908215347922299  unrelated_bert_f_score: 0.9952545364697775 \n",
      "idx:  74\n",
      "Concept: Islamic State Validation:  bleu_score: 0.608410194192437  rouge_l_score: 0.7391593592726943  bert_f_score: 0.8669083297252655 \n",
      "Concept: Islamic State Validation:  unrelated_bleu_score: 0.9757368648626743  unrelated_rouge_l_score: 0.9822738063358472  unrelated_bert_f_score: 0.9921417737007141 \n",
      "idx:  75\n",
      "Concept: United Kingdom Validation:  bleu_score: 0.956654004425647  rouge_l_score: 0.9744497557656007  bert_f_score: 0.9832609350031073 \n",
      "Concept: United Kingdom Validation:  unrelated_bleu_score: 0.9305891730621926  unrelated_rouge_l_score: 0.9599679119665209  unrelated_bert_f_score: 0.9722586607933045 \n",
      "idx:  76\n",
      "Concept: Soviet Union Validation:  bleu_score: 0.9027694563997337  rouge_l_score: 0.939467391468234  bert_f_score: 0.9616383639248934 \n",
      "Concept: Soviet Union Validation:  unrelated_bleu_score: 0.9531418286415596  unrelated_rouge_l_score: 0.9716133232653121  unrelated_bert_f_score: 0.9834384790488652 \n",
      "idx:  77\n",
      "Concept: tensorflow Validation:  bleu_score: 0.7807729402207687  rouge_l_score: 0.846980880203733  bert_f_score: 0.9154388666152954 \n",
      "Concept: tensorflow Validation:  unrelated_bleu_score: 0.946742685595971  unrelated_rouge_l_score: 0.9656692583576502  unrelated_bert_f_score: 0.9806547503845364 \n",
      "idx:  78\n",
      "Concept: Ancient Rome Validation:  bleu_score: 0.6243163460454187  rouge_l_score: 0.7246925832896298  bert_f_score: 0.8576886892318726 \n",
      "Concept: Ancient Rome Validation:  unrelated_bleu_score: 0.9601939089233104  unrelated_rouge_l_score: 0.9672867802816739  unrelated_bert_f_score: 0.9846047532558441 \n",
      "idx:  79\n",
      "Concept: Cowboy Validation:  bleu_score: 0.6085670459200343  rouge_l_score: 0.7296796616274461  bert_f_score: 0.8526272237300873 \n",
      "Concept: Cowboy Validation:  unrelated_bleu_score: 0.9727171191521493  unrelated_rouge_l_score: 0.979724790561058  unrelated_bert_f_score: 0.9906314015388489 \n",
      "idx:  80\n",
      "Concept: Cocktail Validation:  bleu_score: 0.5457950722727186  rouge_l_score: 0.6865349929619758  bert_f_score: 0.8362894058227539 \n",
      "Concept: Cocktail Validation:  unrelated_bleu_score: 0.952241078155232  unrelated_rouge_l_score: 0.963662877415719  unrelated_bert_f_score: 0.9793368911743164 \n",
      "idx:  81\n",
      "Concept: Operating system Validation:  bleu_score: 0.9093880480009238  rouge_l_score: 0.9444111253795529  bert_f_score: 0.9747095823287963 \n",
      "Concept: Operating system Validation:  unrelated_bleu_score: 0.9689822494078704  unrelated_rouge_l_score: 0.9766625817851641  unrelated_bert_f_score: 0.9905208003520966 \n",
      "idx:  82\n",
      "Concept: Culture of Africa Validation:  bleu_score: 0.6501374125497482  rouge_l_score: 0.7594289629929876  bert_f_score: 0.8775091648101807 \n",
      "Concept: Culture of Africa Validation:  unrelated_bleu_score: 0.991075031951824  unrelated_rouge_l_score: 0.9943791891728068  unrelated_bert_f_score: 0.9965553552496667 \n",
      "idx:  83\n",
      "Concept: Communism Validation:  bleu_score: 0.5872998233608322  rouge_l_score: 0.704458921340515  bert_f_score: 0.8558401226997375 \n",
      "Concept: Communism Validation:  unrelated_bleu_score: 0.9724913548165228  unrelated_rouge_l_score: 0.9819160719572937  unrelated_bert_f_score: 0.9917973782501969 \n",
      "idx:  84\n",
      "Concept: Muslims Validation:  bleu_score: 0.8349805285034215  rouge_l_score: 0.8881316179236783  bert_f_score: 0.9432308495044708 \n",
      "Concept: Muslims Validation:  unrelated_bleu_score: 0.8932088371708411  unrelated_rouge_l_score: 0.9321021364356588  unrelated_bert_f_score: 0.9620605594706986 \n",
      "idx:  85\n",
      "Concept: Netherlands Validation:  bleu_score: 0.8326559526124562  rouge_l_score: 0.8886056977114842  bert_f_score: 0.9439768493175507 \n",
      "Concept: Netherlands Validation:  unrelated_bleu_score: 0.9479405562428865  unrelated_rouge_l_score: 0.9659972570157082  unrelated_bert_f_score: 0.9821171263853709 \n",
      "idx:  86\n",
      "Concept: Network socket Validation:  bleu_score: 0.9247152457583582  rouge_l_score: 0.959455123206918  bert_f_score: 0.9748860538005829 \n",
      "Concept: Network socket Validation:  unrelated_bleu_score: 0.9515986417746394  unrelated_rouge_l_score: 0.9697519573268018  unrelated_bert_f_score: 0.9842333924770356 \n",
      "idx:  87\n",
      "Concept: assachusetts Institute of Technology Validation:  bleu_score: 0.9194740557829706  rouge_l_score: 0.963492058558705  bert_f_score: 0.9818982958793641 \n",
      "Concept: assachusetts Institute of Technology Validation:  unrelated_bleu_score: 0.9821123306771817  unrelated_rouge_l_score: 0.9862413541148535  unrelated_bert_f_score: 0.9923685944080353 \n",
      "idx:  88\n",
      "Concept: Wall Street Validation:  bleu_score: 0.9660447975226346  rouge_l_score: 0.9811706787187415  bert_f_score: 0.9910129904747009 \n",
      "Concept: Wall Street Validation:  unrelated_bleu_score: 0.9687139299373139  unrelated_rouge_l_score: 0.9772529660089695  unrelated_bert_f_score: 0.9888715732097626 \n",
      "idx:  89\n",
      "Concept: France Validation:  bleu_score: 0.8938973943185187  rouge_l_score: 0.9208988714050561  bert_f_score: 0.960949593782425 \n",
      "Concept: France Validation:  unrelated_bleu_score: 0.9275045997375045  unrelated_rouge_l_score: 0.9511810782910269  unrelated_bert_f_score: 0.9761175281471677 \n",
      "idx:  90\n",
      "Concept: Wikipedia Validation:  bleu_score: 0.7053241370584235  rouge_l_score: 0.8014936199412656  bert_f_score: 0.8909360647201539 \n",
      "Concept: Wikipedia Validation:  unrelated_bleu_score: 0.9329509605222085  unrelated_rouge_l_score: 0.957852552142044  unrelated_bert_f_score: 0.9770888125896454 \n",
      "idx:  91\n",
      "Concept: Array (data structure) Validation:  bleu_score: 0.3914727112833326  rouge_l_score: 0.5830236937049946  bert_f_score: 0.7810329914093017 \n",
      "Concept: Array (data structure) Validation:  unrelated_bleu_score: 0.9850377650530379  unrelated_rouge_l_score: 0.9903406048753397  unrelated_bert_f_score: 0.9948905408382416 \n",
      "idx:  92\n",
      "Concept: Cantonese Validation:  bleu_score: 0.8526264696102367  rouge_l_score: 0.913544212290287  bert_f_score: 0.9596467912197113 \n",
      "Concept: Cantonese Validation:  unrelated_bleu_score: 0.9834277105125387  unrelated_rouge_l_score: 0.9883475733477813  unrelated_bert_f_score: 0.9931025910377502 \n",
      "idx:  93\n",
      "Concept: Victoria's Secret Validation:  bleu_score: 0.45328126706104455  rouge_l_score: 0.5984343399489953  bert_f_score: 0.7847383379936218 \n",
      "Concept: Victoria's Secret Validation:  unrelated_bleu_score: 0.974043690648495  unrelated_rouge_l_score: 0.9848816968772349  unrelated_bert_f_score: 0.9911944711208344 \n",
      "idx:  94\n",
      "Concept: Jesus Validation:  bleu_score: 0.6183483939561465  rouge_l_score: 0.7330802399232216  bert_f_score: 0.8531449019908905 \n",
      "Concept: Jesus Validation:  unrelated_bleu_score: 0.9738982467740229  unrelated_rouge_l_score: 0.9828541599436349  unrelated_bert_f_score: 0.9918879723107373 \n",
      "idx:  95\n",
      "Concept: Matplotlib Validation:  bleu_score: 0.9785247827450358  rouge_l_score: 0.9860088315243384  bert_f_score: 0.9911650589534214 \n",
      "Concept: Matplotlib Validation:  unrelated_bleu_score: 0.976567441728755  unrelated_rouge_l_score: 0.9850814997025373  unrelated_bert_f_score: 0.9931385719776153 \n",
      "idx:  96\n",
      "Concept: Amazon (company) Validation:  bleu_score: 0.8746957068401523  rouge_l_score: 0.8998626323639524  bert_f_score: 0.9450490653514863 \n",
      "Concept: Amazon (company) Validation:  unrelated_bleu_score: 0.9220570174949955  unrelated_rouge_l_score: 0.9452558684267507  unrelated_bert_f_score: 0.9675996665861092 \n",
      "idx:  97\n",
      "Concept: Amazon Kindle Validation:  bleu_score: 0.8565392027605224  rouge_l_score: 0.9208828417860819  bert_f_score: 0.9623564422130585 \n",
      "Concept: Amazon Kindle Validation:  unrelated_bleu_score: 0.9203978367564688  unrelated_rouge_l_score: 0.9479895725398378  unrelated_bert_f_score: 0.9732922482490539 \n",
      "idx:  98\n",
      "Concept: Amazon Prime Validation:  bleu_score: 0.5441475463294968  rouge_l_score: 0.7067577693507962  bert_f_score: 0.8550510883331299 \n",
      "Concept: Amazon Prime Validation:  unrelated_bleu_score: 0.9469636388224549  unrelated_rouge_l_score: 0.9712399173233842  unrelated_bert_f_score: 0.9834725511074066 \n",
      "idx:  99\n",
      "Concept: Euro Validation:  bleu_score: 0.8412345576857752  rouge_l_score: 0.8908695602193489  bert_f_score: 0.9402765810489655 \n",
      "Concept: Euro Validation:  unrelated_bleu_score: 0.9670017440072854  unrelated_rouge_l_score: 0.9771788684477969  unrelated_bert_f_score: 0.9882937759723304 \n",
      "idx:  100\n",
      "Concept: Korea Validation:  bleu_score: 0.6948952185773357  rouge_l_score: 0.7746582457875759  bert_f_score: 0.8778789877891541 \n",
      "Concept: Korea Validation:  unrelated_bleu_score: 0.9755847689178336  unrelated_rouge_l_score: 0.9807673898149037  unrelated_bert_f_score: 0.9908661246299744 \n",
      "idx:  101\n",
      "Concept: Jimmy Butler Validation:  bleu_score: 0.9961954175410497  rouge_l_score: 0.9989247261828536  bert_f_score: 0.9995756030082703 \n",
      "Concept: Jimmy Butler Validation:  unrelated_bleu_score: 0.9809687093535093  unrelated_rouge_l_score: 0.9876413000901181  unrelated_bert_f_score: 0.9934955115588207 \n",
      "idx:  102\n",
      "Concept: World War II Validation:  bleu_score: 0.7116693502932936  rouge_l_score: 0.8028710457248835  bert_f_score: 0.8977483689785004 \n",
      "Concept: World War II Validation:  unrelated_bleu_score: 0.9594582441601288  unrelated_rouge_l_score: 0.9722406354814316  unrelated_bert_f_score: 0.9827897644042969 \n",
      "idx:  103\n",
      "Concept: Germany Validation:  bleu_score: 0.7820677887903347  rouge_l_score: 0.8554471570303847  bert_f_score: 0.9228869080543518 \n",
      "Concept: Germany Validation:  unrelated_bleu_score: 0.9372072016069376  unrelated_rouge_l_score: 0.9584977434714745  unrelated_bert_f_score: 0.9780383741855622 \n",
      "idx:  104\n",
      "Concept: Netflix Validation:  bleu_score: 0.44170851067963096  rouge_l_score: 0.6307669771178479  bert_f_score: 0.8219739317893981 \n",
      "Concept: Netflix Validation:  unrelated_bleu_score: 0.9742611930124451  unrelated_rouge_l_score: 0.9806527002283529  unrelated_bert_f_score: 0.9900733392972213 \n",
      "idx:  105\n",
      "Concept: Blockchain Validation:  bleu_score: 0.4551147732115739  rouge_l_score: 0.6072786062729414  bert_f_score: 0.8061617136001586 \n",
      "Concept: Blockchain Validation:  unrelated_bleu_score: 0.8400239589034288  unrelated_rouge_l_score: 0.8895406975777999  unrelated_bert_f_score: 0.9385559940338135 \n",
      "idx:  106\n",
      "Concept: Satan Validation:  bleu_score: 0.17434373856517657  rouge_l_score: 0.37930856500631455  bert_f_score: 0.6239388823509217 \n",
      "Concept: Satan Validation:  unrelated_bleu_score: 0.9708759021546418  unrelated_rouge_l_score: 0.9771154716427084  unrelated_bert_f_score: 0.9881766496920118 \n",
      "idx:  107\n",
      "Concept: Pornography Validation:  bleu_score: 0.6424592127918248  rouge_l_score: 0.7485510239584074  bert_f_score: 0.8691485941410064 \n",
      "Concept: Pornography Validation:  unrelated_bleu_score: 0.9289072484518762  unrelated_rouge_l_score: 0.9542489739941754  unrelated_bert_f_score: 0.979048527636618 \n",
      "idx:  108\n",
      "Concept: Catholic Church Validation:  bleu_score: 0.6492173541396324  rouge_l_score: 0.7506743370406324  bert_f_score: 0.8789147734642029 \n",
      "Concept: Catholic Church Validation:  unrelated_bleu_score: 0.8909118018998179  unrelated_rouge_l_score: 0.9255924335199699  unrelated_bert_f_score: 0.9523498153686524 \n",
      "idx:  109\n",
      "Concept: Heroin Validation:  bleu_score: 0.5720475339260367  rouge_l_score: 0.693936148288435  bert_f_score: 0.8376182734966278 \n",
      "Concept: Heroin Validation:  unrelated_bleu_score: 0.9583046483272624  unrelated_rouge_l_score: 0.9697202789723484  unrelated_bert_f_score: 0.9848870301246643 \n",
      "idx:  110\n",
      "Concept: Native Americans in the United States Validation:  bleu_score: 0.4944834112395079  rouge_l_score: 0.6360480514604585  bert_f_score: 0.796321588754654 \n",
      "Concept: Native Americans in the United States Validation:  unrelated_bleu_score: 0.9088834664471486  unrelated_rouge_l_score: 0.933885339207099  unrelated_bert_f_score: 0.9633266166144726 \n",
      "idx:  111\n",
      "Concept: Pornography Validation:  bleu_score: 0.9492909279656946  rouge_l_score: 0.9642424192424753  bert_f_score: 0.9808747351169587 \n",
      "Concept: Pornography Validation:  unrelated_bleu_score: 0.9514023462264921  unrelated_rouge_l_score: 0.9669959446987573  unrelated_bert_f_score: 0.9848197466797299 \n",
      "idx:  112\n",
      "Concept: United Kingdom Validation:  bleu_score: 0.6984553247394543  rouge_l_score: 0.8188313552858897  bert_f_score: 0.9064234077930451 \n",
      "Concept: United Kingdom Validation:  unrelated_bleu_score: 0.9347870136197816  unrelated_rouge_l_score: 0.9573953997135909  unrelated_bert_f_score: 0.9773863927990782 \n",
      "idx:  113\n",
      "Concept: Jewish culture Validation:  bleu_score: 0.6854879965590405  rouge_l_score: 0.777693519941072  bert_f_score: 0.8764394342899322 \n",
      "Concept: Jewish culture Validation:  unrelated_bleu_score: 0.8779703116043678  unrelated_rouge_l_score: 0.9154751442446121  unrelated_bert_f_score: 0.9585719487883828 \n",
      "idx:  114\n",
      "Concept: Astrological sign Validation:  bleu_score: 0.2389460013548501  rouge_l_score: 0.4517749538336627  bert_f_score: 0.6349646627902985 \n",
      "Concept: Astrological sign Validation:  unrelated_bleu_score: 0.9480535720881893  unrelated_rouge_l_score: 0.9611481122146042  unrelated_bert_f_score: 0.9785328876972198 \n",
      "idx:  115\n",
      "Concept: Turkey Validation:  bleu_score: 0.9628982937528905  rouge_l_score: 0.972152842154615  bert_f_score: 0.9884716272354126 \n",
      "Concept: Turkey Validation:  unrelated_bleu_score: 0.8532066234077862  unrelated_rouge_l_score: 0.900174366950263  unrelated_bert_f_score: 0.948324420452118 \n",
      "idx:  116\n",
      "Concept: Italy Validation:  bleu_score: 0.8329917733221721  rouge_l_score: 0.8950489186977298  bert_f_score: 0.9462242662906647 \n",
      "Concept: Italy Validation:  unrelated_bleu_score: 0.9635789648265412  unrelated_rouge_l_score: 0.9757462915555289  unrelated_bert_f_score: 0.9864066541194916 \n",
      "idx:  117\n",
      "Concept: Bible Validation:  bleu_score: 0.6981644620510892  rouge_l_score: 0.7802518411374036  bert_f_score: 0.8826988935470581 \n",
      "Concept: Bible Validation:  unrelated_bleu_score: 0.9934414976365753  unrelated_rouge_l_score: 0.9959079233887468  unrelated_bert_f_score: 0.9985122799873352 \n",
      "idx:  118\n",
      "Concept: Matplotlib Validation:  bleu_score: 0.3587651347159556  rouge_l_score: 0.5473452138815613  bert_f_score: 0.766926337372173 \n",
      "Concept: Matplotlib Validation:  unrelated_bleu_score: 0.9730455182065021  unrelated_rouge_l_score: 0.9904988653670581  unrelated_bert_f_score: 0.9939422607421875 \n",
      "idx:  119\n",
      "Concept: Amazon (company) Validation:  bleu_score: 0.8732993881535218  rouge_l_score: 0.9215166765444831  bert_f_score: 0.9398135900497436 \n",
      "Concept: Amazon (company) Validation:  unrelated_bleu_score: 0.8714694126578515  unrelated_rouge_l_score: 0.9061054693655582  unrelated_bert_f_score: 0.9522430243400427 \n"
     ]
    }
   ],
   "source": [
    "with open(\"/root/Unlearn_Harry_Potter/test_data/llama2-7b_Concepts_list_QA.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    concepts_list = json.load(file)\n",
    "\n",
    "#concepts = [\"Amazon Alexa\", \"Harry Potter\", \"The Lord of the Rings\", \"Super Mario\", \"Star Wars\", \"William Shakespeare\", \"Sherlock Holmes\", \"Netflix\", \"Blockchain\", \"Satan\"]\n",
    "#concepts = [\"William Shakespeare\", \"Netflix\",  \"Satan\"]\n",
    "#concepts_for_random_sample = [\"William Shakespeare\", \"Netflix\",  \"Satan\"]\n",
    "\n",
    "list(dict.values())[0]\n",
    "\n",
    "for idx,concept in enumerate(concepts_list):\n",
    "    bleu_scores = []\n",
    "    rouge_l_scores = []\n",
    "    bert_f_scores = []\n",
    "    unrelated_bleu_scores = []\n",
    "    unrelated_rouge_l_scores = []\n",
    "    unrelated_bert_f_scores = []\n",
    "    print(\"idx: \",idx)\n",
    "    \n",
    "    \n",
    "    concept_name = list(concept.keys())[0]\n",
    "    \n",
    "    concept_content = concept[concept_name]\n",
    "    dimension, layer = concept_content['dimension'], concept_content['layer']\n",
    "    Questions = concept['QA']\n",
    "\n",
    "    # concepts_for_random_sample.remove(concept)\n",
    "    # random_selection = random.sample(concepts_for_random_sample, 2)\n",
    "    # concepts_for_random_sample.append(concept)\n",
    "\n",
    "    Questions_unrelated = []\n",
    "\n",
    "    random_selection = random_select_except(concepts_list, 5, idx)\n",
    "    \n",
    "    for selection in random_selection:\n",
    "        Questions_unrelated += selection['QA']\n",
    "     \n",
    "    original_answers, original_unrelated_answers = answers_generate(Questions, Questions_unrelated, noise = 0, location = [dimension, layer])\n",
    "    perturbed_answers, perturbed_unrelated_answers = answers_generate(Questions, Questions_unrelated, noise = 0.1, location = [dimension, layer])\n",
    "    #perturbed_answers_big = answers_generate(Questions, noise = 0.3)\n",
    "\n",
    "    for perturbed_answer, original_answer in zip(perturbed_answers, original_answers):\n",
    "        bleu_scores.append(calculate_bleu(perturbed_answer, original_answer))\n",
    "        rouge_l_scores.append(calculate_rouge_l(perturbed_answer, original_answer))\n",
    "        bert_f_scores.append(calculate_bert_f(perturbed_answer, original_answer))\n",
    "\n",
    "   \n",
    "    for perturbed_unrelated_answer, original_unrelated_answer in zip(perturbed_unrelated_answers, original_unrelated_answers):\n",
    "        unrelated_bleu_scores.append(calculate_bleu(perturbed_unrelated_answer, original_unrelated_answer))\n",
    "        unrelated_rouge_l_scores.append(calculate_rouge_l(perturbed_unrelated_answer, original_unrelated_answer))\n",
    "        unrelated_bert_f_scores.append(calculate_bert_f(perturbed_unrelated_answer, original_unrelated_answer))\n",
    "    \n",
    "\n",
    "    bleu_score = statistics.mean(bleu_scores)  \n",
    "    rouge_l_score = statistics.mean(rouge_l_scores)  \n",
    "    \n",
    "    bert_f_scores = [tensor.item() for tensor in bert_f_scores]\n",
    "    bert_f_score = statistics.mean(bert_f_scores)  \n",
    "\n",
    "    unrelated_bleu_score = statistics.mean(unrelated_bleu_scores)  \n",
    "    unrelated_rouge_l_score = statistics.mean(unrelated_rouge_l_scores) \n",
    "\n",
    "    unrelated_bert_f_scores = [tensor.item() for tensor in unrelated_bert_f_scores]\n",
    "    unrelated_bert_f_score = statistics.mean(unrelated_bert_f_scores) \n",
    "    \n",
    "    print(f\"Concept: {concept_name} Validation: \", f\"bleu_score: {bleu_score} \", f\"rouge_l_score: {rouge_l_score} \", f\"bert_f_score: {bert_f_score} \")\n",
    "    print(f\"Concept: {concept_name} Validation: \", f\"unrelated_bleu_score: {unrelated_bleu_score} \", f\"unrelated_rouge_l_score: {unrelated_rouge_l_score} \", f\"unrelated_bert_f_score: {unrelated_bert_f_score} \")\n",
    "    #print(f\"Concept: {concept_name} Validation: \", f\"bleu_score: {bleu_score} \", f\"rouge_l_score: {rouge_l_score} \")\n",
    "    #print(f\"Concept: {concept_name} Validation: \", f\"unrelated_bleu_score: {unrelated_bleu_score} \", f\"unrelated_rouge_l_score: {unrelated_rouge_l_score} \")\n",
    "    row_data = [idx, concept_name, bleu_score, unrelated_bleu_score, rouge_l_score, unrelated_rouge_l_score, bert_f_score, unrelated_bert_f_score,str(original_answers),str(perturbed_answers), str(original_unrelated_answers),str(perturbed_unrelated_answers)]\n",
    "    ws.append(row_data)    \n",
    " \n",
    "wb.save(\"llama2_7b_validation_full.xlsx\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561da892-0f93-4e2c-abdc-ba5ac0764b9a",
   "metadata": {},
   "source": [
    "### Plots for Concepts Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8d101bb-3bf4-43e4-86bb-40223ecaf169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 2]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def random_select_except(lst, n, exclude_index):\n",
    "    # 排除指定位置的元素\n",
    "    candidates = [elem for i, elem in enumerate(lst) if i != exclude_index]\n",
    "    # 从候选元素中随机选择 n 个\n",
    "    selected = random.sample(candidates, n)\n",
    "    return selected\n",
    "\n",
    "# 示例用法\n",
    "my_list = [1, 2, 3, 4, 5]\n",
    "str(\n",
    "exclude_index = 2  # 排除第三个位置的元素\n",
    "selected_elements = random_select_except(my_list, 2, exclude_index)\n",
    "print(selected_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fb1d86f-2640-46e4-94e6-e660a8300b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1, 2, 3, 4, 5]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = [1, 2, 3, 4, 5]\n",
    "str(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db122690-f3c0-4dd2-8556-dd43e876199d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
