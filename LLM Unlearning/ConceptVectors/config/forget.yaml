model_family: llama2-7b #olmo-7b
model_path: /root/data/transformers/Llama-2-7b-chat-hf
data_path:  /root/ConceptVectors/ConceptVectors_data
results_save_path: /root/data/unlearn_results/${model_family}/${forget_loss}
save_dir: /root/data/unlearn_results/1GPU_${forget_loss}_${lr}_${split}_epoch${num_epochs}_batch${batch_size}_accum${gradient_accumulation_steps}_beta${beta}_ref${ref_policy}_eval${eval_steps}_seed${seed}_${run_index}

set: test #dev or test
order: 10
LoRA:
  r: 0
  alpha: 32
  dropout: 0.05

lr: 5e-20 #5e-4  lr should be bigger on Niddle
split: wikipedia #wikipedia, pretraining_data
batch_size: 1
gradient_accumulation_steps: 16
gradient_checkpointing: False  #gradient_checkpointing is banned for olmo-7b
num_epochs: 10
forget_loss: npo_KL  # type: grad_ascent, grad_diff, npo, npo_grad_diff, npo_KL, dpo
ft_type: Full #Full, Sparse, MEMIT, all_value_vectors, Niddle,
loss_threshold: -100

npo_coeff: 1.0
grad_diff_coeff: 1.0
KL_coeff: 1.0
ref_policy: fine_tuned
beta: 0.1
weight_decay: 0.01

seed: 999
run_index: 1
overwrite_dir: True
eval_steps: 1000000000
warmup_steps: steps_per_epoch

